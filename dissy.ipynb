{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "18ye21aUJuyL",
        "kzYmorFbOby_",
        "Z21e1B9_OKf1"
      ],
      "mount_file_id": "1_avgn-aBGMXGtPD7MRZBkiQvN9Z7K8MY",
      "authorship_tag": "ABX9TyPgSO46LUe0BdZQg35qM5Lt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AloraTab/aat2000/blob/main/dissy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "18ye21aUJuyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pandas==1.5.3\n",
        "%pip install tensorflow-addons==0.19.0\n"
      ],
      "metadata": {
        "id": "igoiIt__W5xU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29f662a-f9e1-4325-9fe4-5fe6ed5651c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==1.5.3\n",
            "  Downloading pandas-1.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.5.3) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas==1.5.3) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.4.4\n",
            "    Uninstalling pandas-1.4.4:\n",
            "      Successfully uninstalled pandas-1.4.4\n",
            "Successfully installed pandas-1.5.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons==0.19.0\n",
            "  Downloading tensorflow_addons-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typeguard>=2.7\n",
            "  Downloading typeguard-3.0.2-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons==0.19.0) (23.0)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow-addons==0.19.0) (6.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow-addons==0.19.0) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.7->tensorflow-addons==0.19.0) (3.15.0)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.19.0 typeguard-3.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VU0wpJkMk1O1"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, SeparableConv2D\n",
        "from keras.layers import Attention\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay, CosineDecayRestarts\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imported Classes"
      ],
      "metadata": {
        "id": "kzYmorFbOby_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine Annealing Scheduler\n",
        "# https://github.com/4uiiurz1/keras-cosine-annealing/blob/master/cosine_annealing.py\n",
        "\n",
        "class CosineAnnealingScheduler(Callback):\n",
        "    \"\"\"Cosine annealing scheduler.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
        "        super(CosineAnnealingScheduler, self).__init__()\n",
        "        self.T_max = T_max\n",
        "        self.eta_max = eta_max\n",
        "        self.eta_min = eta_min\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if not hasattr(self.model.optimizer, 'lr'):\n",
        "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "        if self.verbose > 0:\n",
        "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
        "                  'rate to %s.' % (epoch + 1, lr))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
      ],
      "metadata": {
        "id": "atWNoSB1xERL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LR Finder in Keras\n",
        "# https://github.com/avanwyk/tensorflow-projects/tree/master/lr-finder\n",
        "\n",
        "class LRFinder(Callback):\n",
        "    \"\"\"`Callback` that exponentially adjusts the learning rate after each training batch between `start_lr` and\n",
        "    `end_lr` for a maximum number of batches: `max_step`. The loss and learning rate are recorded at each step allowing\n",
        "    visually finding a good learning rate as per https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html via\n",
        "    the `plot` method.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, start_lr: float = 1e-7, end_lr: float = 10, max_steps: int = 100, smoothing=0.9):\n",
        "        super(LRFinder, self).__init__()\n",
        "        self.start_lr, self.end_lr = start_lr, end_lr\n",
        "        self.max_steps = max_steps\n",
        "        self.smoothing = smoothing\n",
        "        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n",
        "        self.lrs, self.losses = [], []\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n",
        "        self.lrs, self.losses = [], []\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        self.lr = self.exp_annealing(self.step)\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, self.lr)\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        logs = logs or {}\n",
        "        loss = logs.get('loss')\n",
        "        step = self.step\n",
        "        if loss:\n",
        "            self.avg_loss = self.smoothing * self.avg_loss + (1 - self.smoothing) * loss\n",
        "            smooth_loss = self.avg_loss / (1 - self.smoothing ** (self.step + 1))\n",
        "            self.losses.append(smooth_loss)\n",
        "            self.lrs.append(self.lr)\n",
        "\n",
        "            if step == 0 or loss < self.best_loss:\n",
        "                self.best_loss = loss\n",
        "\n",
        "            if smooth_loss > 4 * self.best_loss or tf.math.is_nan(smooth_loss):\n",
        "                self.model.stop_training = True\n",
        "\n",
        "        if step == self.max_steps:\n",
        "            self.model.stop_training = True\n",
        "\n",
        "        self.step += 1\n",
        "\n",
        "    def exp_annealing(self, step):\n",
        "        return self.start_lr * (self.end_lr / self.start_lr) ** (step * 1. / self.max_steps)\n",
        "\n",
        "    def plot(self):\n",
        "        fig, ax = plt.subplots(1, 1)\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.set_xlabel('Learning Rate (log scale)')\n",
        "        ax.set_xscale('log')\n",
        "        ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))\n",
        "        ax.plot(self.lrs, self.losses)"
      ],
      "metadata": {
        "id": "PQUyj62yLEFQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/nathanhubens/KerasOneCycle\n",
        "from keras.callbacks import Callback\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#####################\n",
        "##### OneCycle  #####\n",
        "#####################\n",
        "\n",
        "          \n",
        "class OneCycle(Callback):\n",
        "    \"\"\"This callback implements a cyclical learning rate and momentum policy (CLR).\n",
        "    The method cycles the learning rate between two boundaries with\n",
        "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
        "    The amplitude of the cycle can be scaled on a per-iteration \n",
        "    For more detail, please see paper.\n",
        "    \n",
        "    # Example\n",
        "        ```python\n",
        "            clr = OneCycle(min_lr=1e-3, max_lr=1e-2,\n",
        "                      min_mtm=0.85, max_mtm=0.95,\n",
        "                      annealing=0.1,step_size=np.ceil((X_train.shape[0]*epochs/batch_size)))\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "    \n",
        "    # Arguments\n",
        "        min_lr: initial learning rate which is the\n",
        "            lower boundary in the cycle.\n",
        "        max_lr: upper boundary in the cycle. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - min_lr).\n",
        "        step_size: number of training iterations in the cycle. To define as `np.ceil((X_train.shape[0]*epochs/batch_size))`\n",
        "        max_mtm : initial value of the momentum    \n",
        "        min_mtm : lower boundary in the cycle.\n",
        "        annealing_stage : percentage of the iterations where the lr\n",
        "                    will decrease lower than its min_lr\n",
        "        annealing_rate : in annealing phase learning rate will be decreased to annealing_rate*min_lr\n",
        "                    \n",
        "        # References\n",
        "        Original paper: https://arxiv.org/pdf/1803.09820.pdf\n",
        "        Inspired by : https://sgugger.github.io/the-1cycle-policy.html#the-1cycle-policy\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, min_lr=1e-5, max_lr=1e-2, min_mtm = 0.85, max_mtm=0.95, training_iterations=1000.,\n",
        "                 annealing_stage=0.1, annealing_rate=0.01):\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.min_mtm = min_mtm\n",
        "        self.max_mtm = max_mtm\n",
        "        self.annealing_stage = annealing_stage\n",
        "        self.step_size = training_iterations*(1-self.annealing_stage)/2\n",
        "        self.min_annealing_lr = annealing_rate * min_lr\n",
        "        self.iterations = 0.\n",
        "        self.training_iterations = training_iterations\n",
        "        self.history = {}\n",
        "        \n",
        "    def clr(self):\n",
        "        if self.iterations < 2*self.step_size :\n",
        "            x = np.abs(self.iterations/self.step_size - 1)\n",
        "            return self.min_lr + (self.max_lr-self.min_lr)*(1-x)\n",
        "        else:\n",
        "            x = min(1, float(self.iterations - 2 * self.step_size) / (self.training_iterations - 2 * self.step_size))\n",
        "            return self.min_lr - (self.min_lr - self.min_annealing_lr) * x\n",
        "        \n",
        "    \n",
        "    def cmtm(self):\n",
        "        if self.iterations < 2*self.step_size :   \n",
        "            x = np.abs(self.iterations/self.step_size - 1)\n",
        "        else: \n",
        "            x=1\n",
        "        return self.min_mtm + (self.max_mtm-self.min_mtm)*(x)     \n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
        "        K.set_value(self.model.optimizer.momentum, self.max_mtm)\n",
        "         \n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.iterations += 1\n",
        "    \n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('momentum', []).append(K.get_value(self.model.optimizer.momentum))\n",
        "        self.history.setdefault('iterations', []).append(self.iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr()) \n",
        "        K.set_value(self.model.optimizer.momentum, self.cmtm())\n",
        "        \n",
        "    def plot_lr(self):\n",
        "        plt.xlabel('Training Iterations')\n",
        "        plt.ylabel('Learning Rate')\n",
        "        plt.title(\"CLR - '1 cycle' Policy\")\n",
        "        plt.plot(self.history['iterations'], self.history['lr'])\n",
        "        \n",
        "    def plot_mtm(self):\n",
        "        plt.xlabel('Training Iterations')\n",
        "        plt.ylabel('Momentum')\n",
        "        plt.title(\"CLR - '1 cycle' Policy\")\n",
        "        plt.plot(self.history['iterations'], self.history['momentum'])"
      ],
      "metadata": {
        "id": "Hrbft0GWglE5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Dataset"
      ],
      "metadata": {
        "id": "Tagc4GM2KCxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD1v8irVVL8A",
        "outputId": "2218f37e-0328-49d2-dda7-f50e727f023c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/gdrive/MyDrive/Colab Notebooks/Dataset'\n",
        "fileName = 'csi-dataset-20-7-5-newpreprocwfil.pkl'\n",
        "fullPath = os.path.join(PATH, fileName)\n",
        "print(fullPath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5-JT2q-WN6F",
        "outputId": "6202b4fb-5892-4a3e-a7e2-9e2ff16d0c43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/Dataset/csi-dataset-20-7-5-newpreprocwfil.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_pickle(fullPath)"
      ],
      "metadata": {
        "id": "qPFc8j7Sl_AE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = df.sample(frac=1)\n",
        "Y = df['Label'].values\n",
        "X = df['Sample'].values\n",
        "X = [[np.asarray(sample) for sample in i] for i in X]\n",
        "X = np.asarray(X).astype('float32')"
      ],
      "metadata": {
        "id": "9OTolodRQ-6R"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqNzG8VaOtPD",
        "outputId": "9c61d212-e027-479f-fbef-30c775b86568"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6642, 256, 2, 3, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unique, counts = np.unique(Y_test, return_counts=True)\n",
        "# print(np.asarray((unique, counts)).T)"
      ],
      "metadata": {
        "id": "mpSnjlqxJRGZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "vectorized_y = np_utils.to_categorical(encoded_Y)"
      ],
      "metadata": {
        "id": "4nj7Tr5NLTgl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = encoder.classes_\n",
        "print(classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAFDwJkoAN7s",
        "outputId": "841745a4-7799-47a0-e000-098adc30ae3c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I1' 'I11' 'I13' 'I3' 'I5' 'I7' 'I9']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,vectorized_y, test_size=0.3, random_state=40)"
      ],
      "metadata": {
        "id": "vZpv6d3uRBgV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1],X_train.shape[2],1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1],X_test.shape[2],1))\n",
        "\n",
        "X_train = tf.convert_to_tensor(X_train)\n",
        "Y_train = tf.convert_to_tensor(Y_train)"
      ],
      "metadata": {
        "id": "UibBIJWtRCkL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "a861cc93-243c-41fd-c565-36135eeb040f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e5de67357165>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 214225920 into shape (4649,256,2,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Model"
      ],
      "metadata": {
        "id": "EmE5iAnfKuK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Components"
      ],
      "metadata": {
        "id": "Z21e1B9_OKf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual Connections\n",
        "# https://stackoverflow.com/questions/64792460/how-to-code-a-residual-block-using-two-layers-of-a-basic-cnn-algorithm-built-wit\n",
        "\n",
        "def resblock(x, kernelsize, filters):\n",
        "    fx = keras.layers.Conv2D(filters, kernelsize, activation='relu', padding='same')(x)\n",
        "    fx = keras.layers.BatchNormalization()(fx)\n",
        "    fx = keras.layers.Conv2D(filters, kernelsize, padding='same')(fx)\n",
        "    out = keras.layers.Add()([x,fx])\n",
        "    out = keras.layers.ReLU()(out)\n",
        "    out = keras.layers.BatchNormalization()(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "5B8Uwo10OfQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mish Activation\n",
        "def mish(inputs):\n",
        "    x = tf.nn.softplus(inputs)\n",
        "    x = tf.nn.tanh(x)\n",
        "    x = tf.multiply(x,inputs)\n",
        "    return x"
      ],
      "metadata": {
        "id": "CDOv6JpKRG47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ranger Optimizer\n",
        "# www.kaggle.com/code/yazanmajzob/ranger-optimizeranvas.com\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "def Ranger(sync_period=6,\n",
        "           slow_step_size=0.5,\n",
        "           learning_rate=0.001,\n",
        "           beta_1=0.9,\n",
        "           beta_2=0.999,\n",
        "           epsilon=1e-7,\n",
        "           weight_decay=0.,\n",
        "           amsgrad=False,\n",
        "           sma_threshold=5.0,\n",
        "           total_steps=0,\n",
        "           warmup_proportion=0.1,\n",
        "           min_lr=0.,\n",
        "           name=\"Ranger\"):\n",
        "    inner = tfa.optimizers.RectifiedAdam(learning_rate, beta_1, beta_2, epsilon, weight_decay, amsgrad, sma_threshold, total_steps, warmup_proportion, min_lr, name)\n",
        "    optim = tfa.optimizers.Lookahead(inner, sync_period, slow_step_size, name)\n",
        "    return optim"
      ],
      "metadata": {
        "id": "Kmg26DSAG_J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Building"
      ],
      "metadata": {
        "id": "Au5bnuTWOUnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basecsiTime_block(input_layer):\n",
        "    # layer_1 = Conv2D(10, (1,1), padding='same', activation='relu')(input_layer)\n",
        "    layer_1 = Conv2D(32, (10,10), padding='same', activation='relu')(input_layer)\n",
        "    # layer_1 = SeparableConv2D(32, (10,10), padding='same', activation='relu')(input_layer)\n",
        "\n",
        "    # layer_2 = Conv2D(10, (1,1), padding='same', activation='relu')(input_layer)\n",
        "    layer_2 = Conv2D(32, (20,20), padding='same', activation='relu')(input_layer)\n",
        "    # layer_2 = SeparableConv2D(32, (20,20), padding='same', activation='relu')(input_layer)\n",
        "\n",
        "    # layer_3 = Conv2D(10, (1,1), padding='same')(input_layer)\n",
        "    layer_3 = Conv2D(32, (40,40), padding='same', activation='relu')(input_layer)\n",
        "    # layer_3 = SeparableConv2D(32, (40,40), padding='same', activation='relu')(input_layer)\n",
        "\n",
        "    layer_4 = keras.layers.MaxPooling2D(pool_size=(1,1))(input_layer)\n",
        "\n",
        "    # layer_5 = keras.layers.concatenate([layer_1, layer_2, layer_3, layer_4], axis = 3)\n",
        "    layer_5 = keras.layers.concatenate([layer_1, layer_2, layer_3, layer_4])\n",
        "\n",
        "    layer_6 = keras.layers.BatchNormalization()(layer_5)\n",
        "    output_layer = keras.layers.Dense(7, activation=mish)(layer_6)\n",
        "    \n",
        "    return output_layer\n",
        "\n",
        "def csiTime_selfattn(input_layer):\n",
        "    layer_1 = Conv2D(32, (10,10), padding='same', activation='relu')(input_layer)\n",
        "    layer_2 = Conv2D(32, (20,20), padding='same', activation='relu')(input_layer)\n",
        "    layer_3 = Conv2D(32, (40,40), padding='same', activation='relu')(input_layer)\n",
        "\n",
        "    layer_4 = keras.layers.MaxPooling2D(pool_size=(1,1))(input_layer)\n",
        "    layer_5 = keras.layers.concatenate([layer_1, layer_2, layer_3, layer_4])\n",
        "\n",
        "    layer_6 = keras.layers.BatchNormalization()(layer_5)\n",
        "    layer_7 = keras.layers.Attention()([layer_6, layer_6])\n",
        "    output_layer = keras.layers.Dense(7, activation=mish)(layer_7)\n",
        "    \n",
        "    return output_layer\n",
        "\n",
        "def csi_mini(input_layer):\n",
        "    \n",
        "    layer_1 = SeparableConv2D(32, (10,10), padding='same', activation='relu')(input_layer)\n",
        "    layer_2 = SeparableConv2D(32, (20,20), padding='same', activation='relu')(input_layer)\n",
        "    layer_3 = SeparableConv2D(32, (40,40), padding='same', activation='relu')(input_layer)\n",
        "\n",
        "    layer_4 = keras.layers.MaxPooling2D(pool_size=(1,1))(input_layer)\n",
        "    layer_5 = keras.layers.concatenate([layer_1, layer_2, layer_3, layer_4])\n",
        "\n",
        "    layer_6 = keras.layers.BatchNormalization()(layer_5)\n",
        "    output_layer = keras.layers.Dense(7, activation=mish)(layer_6)\n",
        "    \n",
        "    return output_layer"
      ],
      "metadata": {
        "id": "3HF2Ga72RIXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.nn_ops import softmax\n",
        "\n",
        "def make_model(input_shape):\n",
        "    input_layer = keras.layers.Input(input_shape, dtype='float32')\n",
        "\n",
        "    fir = basecsiTime_block(input_layer)\n",
        "    sec = basecsiTime_block(fir)\n",
        "    res1 = resblock(sec, (3,3), 7)\n",
        "    thi = csiTime_selfattn(res1)\n",
        "    fou = basecsiTime_block(thi)\n",
        "    fif = csiTime_selfattn(fou)\n",
        "    res2 = resblock(fif, (3,3), 7)\n",
        "    six = basecsiTime_block(res2)\n",
        "\n",
        "    gap = keras.layers.GlobalAveragePooling2D()(six)\n",
        "    # dropout = keras.layers.Dropout(.2)(gap)\n",
        "    # layer_4 = keras.layers.Flatten()(layer_4)\n",
        "    output_layer = keras.layers.Dense(7, activation=softmax)(gap) #Need to change to num of classes\n",
        "    \n",
        "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "model = make_model(input_shape=X_train.shape[1:])\n",
        "# keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "6X8tHx_uRKNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "AHwM0c7sRNnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_size = 48\n",
        "steps_per_epoch = X_train.shape[0]\n",
        "\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=100, decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "# lr_schedule = OneCycle(min_lr=7e-3, max_lr=7e-2, min_mtm = 0.85, max_mtm = 0.95, annealing_stage=0.1, annealing_rate=0.01,\n",
        "#           training_iterations=(X_train.shape[0]*epochs)/(batch_size))\n",
        "# lr_schedule = CosineDecayRestarts(initial_learning_rate=0.001,first_decay_steps=1000)\n",
        "\n",
        "# clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=3e-7,\n",
        "#     maximal_learning_rate=3e-2,\n",
        "#     scale_fn=lambda x: 1/(2.**(x-1)),\n",
        "#     step_size=2 * steps_per_epoch\n",
        "# )\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"basebase.h5\", save_best_only=True, monitor=\"val_loss\"\n",
        "    ),\n",
        "    # OneCycle(min_lr=7e-3, max_lr=7e-2, min_mtm = 0.85, max_mtm = 0.95, annealing_stage=0.1, annealing_rate=0.01,\n",
        "    #       training_iterations=(X_train.shape[0]*epochs)/(batch_size)),\n",
        "    # keras.callbacks.ReduceLROnPlateau(\n",
        "    #     monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
        "    # ),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1),\n",
        "]\n",
        "model.compile(\n",
        "    optimizer=Ranger(learning_rate = lr_schedule),\n",
        "    # optimizer = keras.optimizers.SGD(lr=0.0025, momentum=0.95, nesterov=True),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"categorical_accuracy\"],\n",
        ")\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    Y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.2,\n",
        "    verbose=1,\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "id": "dyDdzsiLRQnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "qpTFulowK0eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model(\"sa-mish-adam-150.h5\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test[:,:,:], Y_test)\n",
        "\n",
        "print(\"Test accuracy\", test_acc)\n",
        "print(\"Test loss\", test_loss)"
      ],
      "metadata": {
        "id": "1RUaJ2nQRUyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = \"categorical_accuracy\"\n",
        "plt.figure()\n",
        "plt.plot(history.history[metric])\n",
        "plt.plot(history.history[\"val_\" + metric])\n",
        "plt.title(\"Baseline's \" + metric)\n",
        "plt.ylabel(metric, fontsize=\"large\")\n",
        "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "FellN_9WRZzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix and Classification Report\n",
        "\n",
        "y_pred=model.predict(X_test) \n",
        "y_pred=np.argmax(y_pred, axis=1)\n",
        "Y_test=np.argmax(Y_test, axis=1)\n",
        "cm = confusion_matrix(Y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "print('Classification Report')\n",
        "print(classification_report(Y_test, y_pred, target_names=classes))"
      ],
      "metadata": {
        "id": "Y235RZ51Q4jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "\n",
        "model.predict(X_test[:10])"
      ],
      "metadata": {
        "id": "XorlWcV3m9ep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}